{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "312c3e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Load the CLIP model and preprocess function\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device)\n",
    "\n",
    "# Define action descriptions (These can be changed or extended)\n",
    "actions = [\n",
    "    \"A person running\",\n",
    "    \"A person jumping\",\n",
    "    \"A person sitting\",\n",
    "    \"A person walking\",\n",
    "    \"A person standing\"\n",
    "]\n",
    "\n",
    "# Encode action descriptions into text embeddings\n",
    "text_inputs = torch.cat([clip.tokenize(action) for action in actions]).to(device)\n",
    "with torch.no_grad():\n",
    "    text_embeddings = model.encode_text(text_inputs)\n",
    "\n",
    "# Normalize the text embeddings to unit vectors\n",
    "text_embeddings /= text_embeddings.norm(dim=-1, keepdim=True)\n",
    "\n",
    "# Initialize video capture from webcam (0 is the default camera on most systems)\n",
    "cap = cv2.VideoCapture(0)  # Use '0' for the built-in webcam\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Unable to access the camera.\")\n",
    "    exit()\n",
    "\n",
    "frame_skip = 3  # Process every 3rd frame\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Failed to capture frame.\")\n",
    "        break\n",
    "    \n",
    "    # Skip frames to reduce processing load\n",
    "    if cap.get(cv2.CAP_PROP_POS_FRAMES) % frame_skip != 0:\n",
    "        continue\n",
    "\n",
    "    # Resize the frame to match CLIP input size (224x224)\n",
    "    frame_resized = cv2.resize(frame, (224, 224))\n",
    "\n",
    "    # Preprocess the frame\n",
    "    pil_image = Image.fromarray(cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB))\n",
    "    image_input = preprocess(pil_image).unsqueeze(0).to(device)\n",
    "\n",
    "    # Get the image embedding from CLIP\n",
    "    with torch.no_grad():\n",
    "        image_embedding = model.encode_image(image_input)\n",
    "\n",
    "    # Normalize the image embedding\n",
    "    image_embedding /= image_embedding.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    # Flatten the image embedding to 1D\n",
    "    image_embedding = image_embedding.cpu().numpy().flatten()\n",
    "\n",
    "    # Convert text embeddings to numpy and flatten them\n",
    "    text_embeddings_np = text_embeddings.cpu().numpy()\n",
    "\n",
    "    # Calculate cosine similarity between image embedding and text embeddings\n",
    "    similarities = []\n",
    "    for text_embedding in text_embeddings_np:\n",
    "        similarity = 1 - cosine(image_embedding, text_embedding.flatten())\n",
    "        similarities.append(similarity)\n",
    "\n",
    "    # Find the index of the most similar action description\n",
    "    most_similar_idx = np.argmax(similarities)\n",
    "    predicted_action = actions[most_similar_idx]\n",
    "\n",
    "    # Display the action on the frame\n",
    "    cv2.putText(frame, f\"Predicted Action: {predicted_action}\", (10, 30),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # Show the frame\n",
    "    cv2.imshow('Real-Time Action Recognition', frame)\n",
    "\n",
    "    # Exit on pressing 'q'\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ab0853c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captured frame: (480, 640, 3)\n",
      "Predicted Action: A person sitting\n",
      "Captured frame: (480, 640, 3)\n",
      "Predicted Action: A person standing\n",
      "Captured frame: (480, 640, 3)\n",
      "Predicted Action: A person sitting\n",
      "Captured frame: (480, 640, 3)\n",
      "Predicted Action: A person standing\n",
      "Captured frame: (480, 640, 3)\n",
      "Predicted Action: A person sitting\n",
      "Captured frame: (480, 640, 3)\n",
      "Predicted Action: A person sitting\n",
      "Captured frame: (480, 640, 3)\n",
      "Predicted Action: A person sitting\n",
      "Captured frame: (480, 640, 3)\n",
      "Predicted Action: A person sitting\n",
      "Captured frame: (480, 640, 3)\n",
      "Predicted Action: A person sitting\n",
      "Captured frame: (480, 640, 3)\n",
      "Predicted Action: A person sitting\n",
      "Captured frame: (480, 640, 3)\n",
      "Predicted Action: A person sitting\n",
      "Captured frame: (480, 640, 3)\n",
      "Predicted Action: A person sitting\n",
      "Captured frame: (480, 640, 3)\n",
      "Predicted Action: A person sitting\n",
      "Captured frame: (480, 640, 3)\n",
      "Predicted Action: A person sitting\n",
      "Captured frame: (480, 640, 3)\n",
      "Predicted Action: A person sitting\n",
      "Captured frame: (480, 640, 3)\n",
      "Predicted Action: A person standing\n",
      "Captured frame: (480, 640, 3)\n",
      "Predicted Action: A person sitting\n",
      "Captured frame: (480, 640, 3)\n",
      "Predicted Action: A person sitting\n",
      "Captured frame: (480, 640, 3)\n",
      "Predicted Action: A person sitting\n",
      "Captured frame: (480, 640, 3)\n",
      "Predicted Action: A person sitting\n",
      "Captured frame: (480, 640, 3)\n",
      "Predicted Action: A person sitting\n",
      "Captured frame: (480, 640, 3)\n",
      "Predicted Action: A person standing\n",
      "Captured frame: (480, 640, 3)\n",
      "Predicted Action: A person standing\n",
      "Captured frame: (480, 640, 3)\n",
      "Predicted Action: A person standing\n",
      "Captured frame: (480, 640, 3)\n",
      "Predicted Action: A person standing\n",
      "Captured frame: (480, 640, 3)\n",
      "Predicted Action: A person standing\n",
      "Captured frame: (480, 640, 3)\n",
      "Predicted Action: A person standing\n",
      "Captured frame: (480, 640, 3)\n",
      "Predicted Action: A person jumping\n",
      "Captured frame: (480, 640, 3)\n",
      "Predicted Action: A person sitting\n",
      "Captured frame: (480, 640, 3)\n",
      "Predicted Action: A person sitting\n",
      "Captured frame: (480, 640, 3)\n",
      "Predicted Action: A person sitting\n",
      "Captured frame: (480, 640, 3)\n",
      "Predicted Action: A person sitting\n",
      "Captured frame: (480, 640, 3)\n",
      "Predicted Action: A person sitting\n",
      "Captured frame: (480, 640, 3)\n",
      "Predicted Action: A person sitting\n",
      "Captured frame: (480, 640, 3)\n",
      "Predicted Action: A person standing\n",
      "Captured frame: (480, 640, 3)\n",
      "Predicted Action: A person standing\n",
      "Captured frame: (480, 640, 3)\n",
      "Predicted Action: A person sitting\n",
      "Captured frame: (480, 640, 3)\n",
      "Predicted Action: A person sitting\n",
      "Captured frame: (480, 640, 3)\n",
      "Predicted Action: A person sitting\n",
      "Captured frame: (480, 640, 3)\n",
      "Predicted Action: A person sitting\n",
      "Captured frame: (480, 640, 3)\n",
      "Predicted Action: A person sitting\n",
      "Captured frame: (480, 640, 3)\n",
      "Predicted Action: A person sitting\n",
      "Captured frame: (480, 640, 3)\n",
      "Predicted Action: A person sitting\n",
      "Captured frame: (480, 640, 3)\n",
      "Predicted Action: A person sitting\n",
      "Captured frame: (480, 640, 3)\n",
      "Predicted Action: A person sitting\n",
      "Captured frame: (480, 640, 3)\n",
      "Predicted Action: A person sitting\n",
      "Captured frame: (480, 640, 3)\n",
      "Predicted Action: A person sitting\n",
      "Captured frame: (480, 640, 3)\n",
      "Predicted Action: A person sitting\n",
      "Captured frame: (480, 640, 3)\n",
      "Predicted Action: A person sitting\n",
      "Captured frame: (480, 640, 3)\n",
      "Predicted Action: A person sitting\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import clip\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Load the CLIP model and preprocessing function\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device)\n",
    "\n",
    "# Define action descriptions\n",
    "actions = [\n",
    "    \"A person running\",\n",
    "    \"A person jumping\",\n",
    "    \"A person sitting\",\n",
    "    \"A person walking\",\n",
    "    \"A person standing\"\n",
    "]\n",
    "\n",
    "# Encode action descriptions into text embeddings\n",
    "text_inputs = torch.cat([clip.tokenize(action) for action in actions]).to(device)\n",
    "with torch.no_grad():\n",
    "    text_embeddings = model.encode_text(text_inputs)\n",
    "\n",
    "# Normalize the text embeddings to unit vectors\n",
    "text_embeddings /= text_embeddings.norm(dim=-1, keepdim=True)\n",
    "\n",
    "# Initialize video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Unable to access the camera.\")\n",
    "    exit()\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Check if a frame was successfully captured\n",
    "    if not ret:\n",
    "        print(\"Error: Failed to capture frame.\")\n",
    "        break\n",
    "    \n",
    "    print(f\"Captured frame: {frame.shape}\")  # Debug: print the shape of the captured frame\n",
    "\n",
    "    # Resize the frame to match CLIP input size\n",
    "    frame_resized = cv2.resize(frame, (224, 224))\n",
    "\n",
    "    # Preprocess the frame\n",
    "    pil_image = Image.fromarray(cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB))\n",
    "    image_input = preprocess(pil_image).unsqueeze(0).to(device)\n",
    "\n",
    "    try:\n",
    "        # Get the image embedding from CLIP\n",
    "        with torch.no_grad():\n",
    "            image_embedding = model.encode_image(image_input)\n",
    "\n",
    "        # Normalize the image embedding\n",
    "        image_embedding /= image_embedding.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # Flatten the image embedding to 1D\n",
    "        image_embedding = image_embedding.cpu().numpy().flatten()\n",
    "\n",
    "        # Convert text embeddings to numpy and flatten them\n",
    "        text_embeddings_np = text_embeddings.cpu().numpy()\n",
    "\n",
    "        # Calculate cosine similarity between image embedding and text embeddings\n",
    "        similarities = []\n",
    "        for text_embedding in text_embeddings_np:\n",
    "            similarity = 1 - cosine(image_embedding, text_embedding.flatten())\n",
    "            similarities.append(similarity)\n",
    "\n",
    "        # Find the index of the most similar action description\n",
    "        most_similar_idx = np.argmax(similarities)\n",
    "        predicted_action = actions[most_similar_idx]\n",
    "\n",
    "        print(f\"Predicted Action: {predicted_action}\")  # Debug: print the predicted action\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing the frame: {e}\")\n",
    "        continue  # Skip the frame if there's an error and move to the next one\n",
    "\n",
    "    # Display the action on the frame\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    frame_with_text = cv2.putText(frame, f\"Predicted Action: {predicted_action}\", (10, 30),\n",
    "                                  font, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "    # Show the frame\n",
    "    cv2.imshow('Real-Time Action Recognition', frame_with_text)\n",
    "\n",
    "    # Exit on pressing 'q'\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
